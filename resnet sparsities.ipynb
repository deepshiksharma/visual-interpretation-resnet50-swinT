{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4392c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4088bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-3\n",
    "\n",
    "def spatial_activation_sparsity(activation, threshold=threshold):\n",
    "    # Element-wise sparsity: count near-zero elements\n",
    "    total_elements = activation.numel()\n",
    "    zero_elements = (activation.abs() < threshold).sum().item()\n",
    "    return zero_elements / total_elements\n",
    "\n",
    "\n",
    "def channel_activation_sparsity(activation, threshold=threshold):\n",
    "    # Mean activation per channel: shape [B, C, H, W] -> [B, C]\n",
    "    per_channel_mean = activation.mean(dim=(2, 3))  # mean over H and W\n",
    "    # Count how many channels have near-zero mean activation\n",
    "    sparse_channels = (per_channel_mean.abs() < threshold).float().mean().item()\n",
    "    return sparse_channels\n",
    "\n",
    "\n",
    "def compute_spatial_activation_sparsity(spatial_maps, threshold=threshold):\n",
    "    sparsity = {}\n",
    "    for name, fmap in spatial_maps.items():  # fmap: [B, H, W]\n",
    "        sparse_ratio = (fmap.abs() < threshold).float().mean().item()\n",
    "        sparsity[name] = sparse_ratio\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "def compute_channel_activation_sparsity(channel_maps, threshold=threshold):\n",
    "    sparsity = {}\n",
    "    for name, fmap in channel_maps.items():  # fmap: [B, C]\n",
    "        sparse_ratio = (fmap.abs() < threshold).float().mean().item()\n",
    "        sparsity[name] = sparse_ratio\n",
    "    return sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a8f30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7761f2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 1000\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../aircraft_dataset\"\n",
    "batch_size = 16\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_path = os.path.join(dataset_path, f\"test\")\n",
    "test_ds = ImageFolder(root=test_path, transform=transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_classes = len(test_ds.classes)\n",
    "\n",
    "print(num_classes, len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb83b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50_InternalRepresentation\n",
      "model parameters: 23651462\n"
     ]
    }
   ],
   "source": [
    "from instrumented_models.resnet_instrumented import ResNet50_InternalRepresentation\n",
    "\n",
    "model = ResNet50_InternalRepresentation(num_classes=num_classes, pretrained=False)\n",
    "\n",
    "weights = torch.load(\"resnet_epoch_25.pth\", map_location=device)\n",
    "weights = {\"model.\" + k: v for k, v in weights.items()}\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(model.__class__.__name__)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"model parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b065173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating test set: 100%|██████████| 63/63 [00:54<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spatial Activation Sparsity:\n",
      "conv1:\t0.0054\n",
      "layer1:\t0.0000\n",
      "layer2:\t0.0000\n",
      "layer3:\t0.0000\n",
      "layer4:\t0.3247\n",
      "\n",
      "Channel Activation Sparsity:\n",
      "conv1:\t0.0077\n",
      "layer1:\t0.0009\n",
      "layer2:\t0.0013\n",
      "layer3:\t0.0002\n",
      "layer4:\t0.2191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_outputs = []\n",
    "all_labels = []\n",
    "all_spatial_maps = defaultdict(list)\n",
    "all_channel_maps = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"evaluating test set\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        spatial_maps, channel_maps = model.get_activation_maps()\n",
    "\n",
    "        for name in spatial_maps:\n",
    "            all_spatial_maps[name].append(spatial_maps[name])\n",
    "            all_channel_maps[name].append(channel_maps[name])\n",
    "\n",
    "        all_outputs.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Concatenate collected feature maps\n",
    "for name in all_spatial_maps:\n",
    "    all_spatial_maps[name] = torch.cat(all_spatial_maps[name], dim=0)\n",
    "    all_channel_maps[name] = torch.cat(all_channel_maps[name], dim=0)\n",
    "\n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Compute both spatial and channel sparsity\n",
    "spatial_sparsity = compute_spatial_activation_sparsity(all_spatial_maps)\n",
    "channel_sparsity = compute_channel_activation_sparsity(all_channel_maps)\n",
    "\n",
    "print(\"\\nSpatial Activation Sparsity:\")\n",
    "for k, v in spatial_sparsity.items():\n",
    "    print(f\"{k}:\\t{v:.4f}\")\n",
    "\n",
    "print(\"\\nChannel Activation Sparsity:\")\n",
    "for k, v in channel_sparsity.items():\n",
    "    print(f\"{k}:\\t{v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1879c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
